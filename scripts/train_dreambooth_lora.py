import os
from pathlib import Path
from PIL import Image
import torch
from tqdm import tqdm

from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler, AutoencoderKL
from transformers import CLIPTokenizer
from peft import LoraConfig, get_peft_model
from diffusers.utils import load_image
from accelerate import Accelerator

# ========== KONFIGURACE ========== #
model_id = "runwayml/stable-diffusion-v1-5"
instance_prompt = "a photo of igloo penguin"
output_dir = "outputs/igloonet_penguin_lora_v2"
data_dir = "data/images_augmented"
resolution = 512
batch_size = 1
learning_rate = 1e-4
num_train_epochs = 6

# ========== INIT ========== #
print("ğŸ“¦ NaÄÃ­tÃ¡m model...")
pipeline = StableDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    safety_checker=None
).to("cuda")

tokenizer = CLIPTokenizer.from_pretrained(model_id)
unet = pipeline.unet
vae = pipeline.vae
scheduler = DDPMScheduler.from_pretrained(model_id, subfolder="scheduler")

# ========== DATA ========== #
print(f"ğŸ“ NaÄÃ­tÃ¡m obrÃ¡zky ze sloÅ¾ky: {data_dir}")
image_paths = list(Path(data_dir).glob("*.png"))
if not image_paths:
    raise ValueError("âŒ Nenalezeny Å¾Ã¡dnÃ© obrÃ¡zky pro trÃ©nink.")
print(f"ğŸ–¼ï¸ PoÄet nalezenÃ½ch obrÃ¡zkÅ¯: {len(image_paths)}")

def load_images():
    images = []
    for path in image_paths:
        img = Image.open(path).convert("RGB").resize((resolution, resolution))
        images.append(img)
    return images

train_images = load_images()
print(f"âœ… NahrÃ¡no {len(train_images)} obrÃ¡zkÅ¯ pro trÃ©nink.")

# ========== PEFT KONFIGURACE ========== #
lora_config = LoraConfig(
    r=4,
    lora_alpha=16,
    target_modules=["to_q", "to_k", "to_v", "to_out.0"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",  # Hack â€“ nebude se pouÅ¾Ã­vat pÅ™Ã­mo
)

print("ğŸ§  PÅ™idÃ¡vÃ¡m LoRA vÃ¡hy do UNet...")
unet = get_peft_model(unet, lora_config)

# ========== TRÃ‰NINK ========== #
optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)
accelerator = Accelerator()

unet, optimizer = accelerator.prepare(unet, optimizer)

unet.train()

print("ğŸš€ ZaÄÃ­nÃ¡m trÃ©nink...")
for epoch in range(num_train_epochs):
    total_loss = 0
    for image in tqdm(train_images, desc=f"Epoch {epoch+1}/{num_train_epochs}"):
        image_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float() / 255.
        image_tensor = image_tensor.to(accelerator.device, dtype=torch.float16)

        with torch.no_grad():
            latents = vae.encode(image_tensor).latent_dist.sample() * 0.18215

        noise = torch.randn_like(latents)
        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (1,), device=latents.device).long()

        noisy_latents = scheduler.add_noise(latents, noise, timesteps)
        encoder_hidden_states = tokenizer(instance_prompt, return_tensors="pt").input_ids.to(latents.device)

        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample
        loss = torch.nn.functional.mse_loss(model_pred, noise)

        accelerator.backward(loss)
        optimizer.step()
        optimizer.zero_grad()
        total_loss += loss.item()

    avg_loss = total_loss / len(train_images)
    print(f"ğŸ“‰ Epoch {epoch+1} dokonÄena. PrÅ¯mÄ›rnÃ¡ loss: {avg_loss:.4f}")

# ========== UKLÃDÃNÃ ========== #
print(f"ğŸ’¾ UklÃ¡dÃ¡m model do sloÅ¾ky: {output_dir}")
pipeline.save_pretrained(output_dir)
print("âœ… TrÃ©nink dokonÄen â€“ jsi pÃ¡n tuÄÅˆÃ¡kÅ¯, LovÄe Å terbin!")
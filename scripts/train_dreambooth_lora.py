import os
import torch
from torch import nn
from torchvision import transforms
from PIL import Image
from pathlib import Path
from tqdm import tqdm
import pandas as pd

from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL
from transformers import CLIPTokenizer
from peft import LoraConfig, get_peft_model

# ========== NastavenÃ­ ==========
model_id = "runwayml/stable-diffusion-v1-5"
instance_prompt = "a penguin"
validation_prompt = "a penguin in igloonet style"

instance_data_dir = "data/images_augmented"
captions_path = "data/captions/captions.csv"
output_dir = "outputs/igloo_penguin_lora"

resolution = 512
batch_size = 1
learning_rate = 1e-4
max_train_steps = 1000
num_epochs = 4

# ========== NaÄtenÃ­ modelu ==========
print("ğŸ“¦ NaÄÃ­tÃ¡m model...")
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")
unet: UNet2DConditionModel = pipe.unet
vae: AutoencoderKL = pipe.vae

# ========== PÅ™idÃ¡nÃ­ LoRA vÃ¡h ==========
print("ğŸ§  PÅ™idÃ¡vÃ¡m LoRA vÃ¡hy do UNet...")
lora_config = LoraConfig(
    r=4,
    lora_alpha=16,
    target_modules=["to_q", "to_k", "to_v", "to_out.0"],
    lora_dropout=0.1,
    bias="none",
    task_type="UNET"
)
unet = get_peft_model(unet, lora_config)

# ========== NaÄtenÃ­ dat ==========
print(f"ğŸ“ NaÄÃ­tÃ¡m obrÃ¡zky ze sloÅ¾ky: {instance_data_dir}")
image_paths = sorted(Path(instance_data_dir).glob("*.png"))
captions_df = pd.read_csv(captions_path)

transform = transforms.Compose([
    transforms.Resize((resolution, resolution)),
    transforms.ToTensor(),
])

def load_images():
    images = []
    for path in image_paths:
        img = Image.open(path).convert("RGB")
        tensor = transform(img).half().unsqueeze(0).to("cuda")
        images.append(tensor)
    return images

train_images = load_images()
print(f"ğŸ–¼ï¸ PoÄet nalezenÃ½ch obrÃ¡zkÅ¯: {len(train_images)}")

# ========== TrÃ©nink (mock) ==========
print("ğŸš€ SpouÅ¡tÃ­m trÃ©nink...")

optimizer = torch.optim.Adam(unet.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    print(f"ğŸ§ª Epoch {epoch + 1}/{num_epochs}")
    for i, (img_tensor, path) in enumerate(zip(train_images, image_paths)):
        optimizer.zero_grad()
        caption = captions_df[captions_df["file_name"] == path.name]["caption"].values[0]

        # Zde by mÄ›lo bÃ½t zakÃ³dovÃ¡nÃ­ promtu pÅ™es text encoder + noise prediction + loss
        # â†’ NahraÄ nÃ¡sledujÃ­cÃ­ Å™Ã¡dek reÃ¡lnÃ½m vÃ½poÄtem loss pÅ™i integraci s plnÃ½m DreamBooth trÃ©ninkem
        loss = torch.rand(1).to("cuda")  # dummy loss

        loss.backward()
        optimizer.step()

        if (i + 1) % 25 == 0:
            print(f"  ğŸ”„ Step {i + 1}/{len(train_images)} - Loss: {loss.item():.4f}")

# ========== UloÅ¾enÃ­ ==========
os.makedirs(output_dir, exist_ok=True)
unet.save_pretrained(output_dir)
print(f"âœ… TrÃ©nink dokonÄen. Model uloÅ¾en do: {output_dir}")